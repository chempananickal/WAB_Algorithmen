{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c57116b2",
   "metadata": {},
   "source": [
    "# Plotting notebook\n",
    "\n",
    "Just a scratchbook to experiment with different plotting ideas and visualizations for the LCS benchmark results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea3d9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Import Required Libraries\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Optional: display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "plt.style.use('seaborn-v0_8')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "184d0a98",
   "metadata": {},
   "source": [
    "### Read the data\n",
    "\n",
    "Adjust paths if necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65daae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Load Raw Benchmark Data\n",
    "raw_path = '../../Code/Results/raw_runs.csv'\n",
    "raw_df = pd.read_csv(raw_path)\n",
    "raw_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d13d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Select Scenario and Algorithm\n",
    "# Example: filter for a scenario and algorithm\n",
    "scenario = 'disjoint_alphabet'  # Change as needed\n",
    "algorithm = 'Enhanced Suffix Array'  # Change as needed\n",
    "\n",
    "filtered_df = raw_df[(raw_df['scenario'] == scenario) & (raw_df['algorithm'] == algorithm)]\n",
    "filtered_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac268882",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Plot Build Time vs String Length\n",
    "plt.figure(figsize=(8, 5))\n",
    "for alg in raw_df['algorithm'].unique():\n",
    "    df = raw_df[(raw_df['scenario'] == scenario) & (raw_df['algorithm'] == alg)]\n",
    "    grouped = df.groupby('length')['build_time_ms'].agg(['median', 'quantile'])\n",
    "    plt.plot(df['length'], df['build_time_ms'], 'o', label=f'{alg} (raw)')\n",
    "plt.xlabel('String length')\n",
    "plt.ylabel('Build time [ms]')\n",
    "plt.title(f'Build Time vs Length ({scenario})')\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8abd90a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Plot Query Time vs String Length\n",
    "plt.figure(figsize=(8, 5))\n",
    "for alg in raw_df['algorithm'].unique():\n",
    "    df = raw_df[(raw_df['scenario'] == scenario) & (raw_df['algorithm'] == alg)]\n",
    "    plt.plot(df['length'], df['query_time_ms'], 'o', label=f'{alg} (raw)')\n",
    "plt.xlabel('String length')\n",
    "plt.ylabel('Query time [ms]')\n",
    "plt.title(f'Query Time vs Length ({scenario})')\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3835b63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Plot Total Time Statistics\n",
    "plt.figure(figsize=(8, 5))\n",
    "for alg in raw_df['algorithm'].unique():\n",
    "    df = raw_df[(raw_df['scenario'] == scenario) & (raw_df['algorithm'] == alg)]\n",
    "    plt.plot(df['length'], df['total_time_ms'], 'o', label=f'{alg} (raw)')\n",
    "plt.xlabel('String length')\n",
    "plt.ylabel('Total time [ms]')\n",
    "plt.title(f'Total Time vs Length ({scenario})')\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae619e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Plot Memory Usage Statistics\n",
    "plt.figure(figsize=(8, 5))\n",
    "for alg in raw_df['algorithm'].unique():\n",
    "    df = raw_df[(raw_df['scenario'] == scenario) & (raw_df['algorithm'] == alg)]\n",
    "    plt.plot(df['length'], df['peak_memory_kib'], 'o', label=f'{alg} (raw)')\n",
    "plt.xlabel('String length')\n",
    "plt.ylabel('Peak memory [KiB]')\n",
    "plt.title(f'Peak Memory vs Length ({scenario})')\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e37a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Plot Index Size and Build/Query Memory\n",
    "plt.figure(figsize=(8, 5))\n",
    "for alg in raw_df['algorithm'].unique():\n",
    "    df = raw_df[(raw_df['scenario'] == scenario) & (raw_df['algorithm'] == alg)]\n",
    "    plt.plot(df['length'], df['index_size_kib'], 'o', label=f'{alg} (index size)')\n",
    "    plt.plot(df['length'], df['build_peak_memory_kib'], 's', label=f'{alg} (build peak)')\n",
    "    plt.plot(df['length'], df['query_extra_memory_kib'], '^', label=f'{alg} (query extra)')\n",
    "plt.xlabel('String length')\n",
    "plt.ylabel('Memory [KiB]')\n",
    "plt.title(f'Index Size and Build/Query Memory vs Length ({scenario})')\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d05771",
   "metadata": {},
   "source": [
    "### Distribution Plot\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "948f602a",
   "metadata": {},
   "outputs": [],
   "source": [
    "length = 10000  # Change as needed (options are 100, 500, 1000, 5000, 10000 if going by raw_runs.csv in the repo)\n",
    "scenario = 'disjoint_alphabet'  # Change as needed (options are \"disjoint_alphabet\", \"mutated_implant\", \"repetitive_with_noise\", \"random_uniform\", and \"near_identical\")\n",
    "\n",
    "algorithms = ['Suffix Automaton', 'Enhanced Suffix Array']\n",
    "\n",
    "metric = 'build_time_ms'  # Change as needed. For options, look at the columns in raw_runs.csv.\n",
    "metric_label = 'Build Time (ms)'\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Suffix Automaton subplot\n",
    "plt.subplot(1, 2, 1)\n",
    "alg = 'Suffix Automaton'\n",
    "df = raw_df[(raw_df['scenario'] == scenario) & (raw_df['algorithm'] == alg) & (raw_df['length'] == length)]\n",
    "plt.hist(df[metric], bins=20, alpha=0.7, color='tab:blue', label=f'{alg} ({metric_label})')\n",
    "plt.xlabel(metric_label)\n",
    "plt.ylabel('Frequency')\n",
    "plt.title(f'Suffix Automaton {metric_label} Distribution')\n",
    "plt.grid(alpha=0.3)\n",
    "\n",
    "# ESA subplot\n",
    "plt.subplot(1, 2, 2)\n",
    "alg = 'Enhanced Suffix Array'\n",
    "df = raw_df[(raw_df['scenario'] == scenario) & (raw_df['algorithm'] == alg) & (raw_df['length'] == length)]\n",
    "plt.hist(df[metric], bins=20, alpha=0.7, color='tab:orange', label=f'{alg} ({metric_label})')\n",
    "plt.xlabel(metric_label)\n",
    "plt.ylabel('Frequency')\n",
    "plt.title(f'ESA {metric_label} Distribution')\n",
    "plt.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7513098",
   "metadata": {},
   "source": [
    "Clearly, everything is heavily skewed, so median would probably be a better \"average\"."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lcs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
